{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model\n",
    "model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",low_cpu_mem_usage=True)\n",
    "model = model.to(\"mps\")  \n",
    "\n",
    "def generate_image(text, save_path):\n",
    "    # Generate the image\n",
    "    with torch.no_grad():\n",
    "        image = model(text).images[0]\n",
    "    # Save the image\n",
    "    image.save(save_path)\n",
    "\n",
    "text = \"palm tree with a sunset behind it\"\n",
    "save_path = \"generated_image1.png\"\n",
    "generate_image(text, save_path)\n",
    "\n",
    "print(f\"Image saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO  # Import BytesIO from io module\n",
    "from diffusers.utils import make_image_grid\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",low_cpu_mem_usage=True)\n",
    "model = model.to(\"mps\")  \n",
    "# Initialize the pipeline\n",
    "# pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "# )\n",
    "\n",
    "# Load and preprocess the initial image\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(init_image)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "# Pass prompt and image to the pipeline\n",
    "output_image = model(prompt, image=image_tensor, strength=0.8).images[0]\n",
    "make_image_grid([init_image, output_image], rows=1, cols=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13232e356304dcd80a65eadf5d61a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6b5363fb0843d1a82ae51cabc013ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers.utils import make_image_grid\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "model = StableDiffusionImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model = model.to(\"mps\")  \n",
    "# Load and preprocess the initial image from a local file\n",
    "file_path = \"/Users/abdullahalsakib/Downloads/Interview/art/server/test.jpeg\"\n",
    "init_image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "# Resize the image to a lower resolution to reduce memory usage\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Boy Crying Sad Cinematic Dark Mental Anger Depression\"\n",
    "\n",
    "# Use torch.no_grad() to reduce memory usage\n",
    "with torch.no_grad():\n",
    "    output_image = model(prompt, image=init_image, strength=0.8).images[0]\n",
    "\n",
    "# Create an image grid\n",
    "grid = make_image_grid([init_image, output_image], rows=1, cols=2)\n",
    "\n",
    "# Save or display the grid\n",
    "grid.save(\"/Users/abdullahalsakib/Downloads/Interview/art/server/output_grid.jpeg\")\n",
    "# Or display using an appropriate method for your environment\n",
    "grid.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Replace the model version with your required version if needed\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Running the inference on GPU with cuda enabled\n",
    "pipeline = pipeline.to('mps')\n",
    "\n",
    "prompt = \"Boy Crying Sad Cinematic Dark Mental Anger Depression\"\n",
    "\n",
    "image = pipeline(prompt=prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, EulerDiscreteScheduler\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "# Model ID\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "# Path to the initial image\n",
    "url = \"/Users/abdullahalsakib/Downloads/Interview/art/server/test.jpeg\"\n",
    "\n",
    "# Load and preprocess the initial image\n",
    "init_images = Image.open(url).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Check device\n",
    "device = \"mps\" \n",
    "\n",
    "# Use the Euler scheduler\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "\n",
    "# Load the pipeline with the specified model and scheduler\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, low_cpu_mem_usage=False)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Define the prompt and other parameters\n",
    "prompt = \"Woman Crying Sad Cinematic Dark Mental Anger Depression\"\n",
    "steps = 20\n",
    "scale = 9\n",
    "num_images_per_prompt = 1\n",
    "seed = torch.randint(0, 1000000, (1,)).item()\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "# Generate the image\n",
    "output = pipe(prompt=prompt, image=init_images, num_inference_steps=steps, guidance_scale=scale, num_images_per_prompt=num_images_per_prompt, generator=generator)\n",
    "\n",
    "# Display the generated image\n",
    "generated_image = output.images[0]\n",
    "display(generated_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
